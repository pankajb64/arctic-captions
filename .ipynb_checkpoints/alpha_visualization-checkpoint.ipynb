{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Visualizing the model  \n",
    "This notebook is meant as an example for how to create visualizations\n",
    "like the ones provided in the appendix.\n",
    "\n",
    "It is expected that this might need some slight modification depending on the user's \n",
    "setup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 960M (CNMeM is enabled with initial size: 45.0% of memory, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "from theano import tensor\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import six.moves.cPickle as pkl\n",
    "import numpy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import skimage\n",
    "import skimage.transform\n",
    "import skimage.io\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import capgen\n",
    "import generate_caps as gencaps\n",
    "import flickr8k\n",
    "import flickr30k\n",
    "import coco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model and dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: flickr30k\n",
      "['COCO_val2014_000000184613.jpg', 'COCO_val2014_000000403013.jpg', 'COCO_val2014_000000562150.jpg', 'COCO_val2014_000000360772.jpg', 'COCO_val2014_000000340559.jpg']\n"
     ]
    }
   ],
   "source": [
    "datasets = {'flickr8k': (flickr8k.load_data, flickr8k.prepare_data),\n",
    "             'flickr30k': (flickr30k.load_data, flickr30k.prepare_data),\n",
    "             'coco': (coco.load_data, coco.prepare_data)}\n",
    "\n",
    "# location of the model file, the pkl file should be named \"model_name.npz.pkl\"\n",
    "model= './model/flickr30k/flickr30k-soft_attn-w512-h1000.npz'\n",
    "# location of the devset split file like the ones in /splits\n",
    "dev_list = './splits/coco_val.txt' \n",
    "image_path = './data/val2014/'\n",
    "\n",
    "\n",
    "# load model model_options\n",
    "with open('%s.pkl'%model, 'rb') as f:\n",
    "    options = pkl.load(f)\n",
    "\n",
    "print ('Loading: ' + options['dataset'])\n",
    "\n",
    "flist = []\n",
    "with open(dev_list, 'r') as f:\n",
    "    for l in f:\n",
    "        flist.append(l.strip())\n",
    "\n",
    "print(flist[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep aspect ratio, and center crop\n",
    "def LoadImage(file_name, resize=256, crop=224):\n",
    "  image = Image.open(file_name)\n",
    "  width, height = image.size\n",
    "\n",
    "  if width > height:\n",
    "    width = (width * resize) // height\n",
    "    height = resize\n",
    "  else:\n",
    "    height = (height * resize) // width\n",
    "    width = resize\n",
    "  left = (width  - crop) // 2\n",
    "  top  = (height - crop) // 2\n",
    "  image_resized = image.resize((width, height), Image.BICUBIC).crop((left, top, left + crop, top + crop))\n",
    "  data = numpy.array(image_resized.convert('RGB').getdata()).reshape(crop, crop, 3)\n",
    "  data = data.astype('float32') / 255\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n",
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "    load_data, prepare_data = datasets[options['dataset']]\n",
    "\n",
    "    train, valid, test, worddict = load_data(False, True, False, path=\"D:\\\\Documents\\\\GitHub\\\\arctic-captions_3\\\\model\\\\flickr30k\\\\\")\n",
    "    print ('Data loaded')\n",
    "\n",
    "    word_idict = dict()\n",
    "    for kk, vv in worddict.items():\n",
    "        word_idict[vv] = kk\n",
    "    word_idict[0] = '<eos>'\n",
    "    word_idict[1] = 'UNK'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Theano Graph  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Building f_init...',)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "    # build the sampling functions and model\n",
    "    trng = RandomStreams(1234)\n",
    "    use_noise = theano.shared(numpy.float32(0.), name='use_noise')\n",
    "\n",
    "    params = capgen.init_params(options)\n",
    "    params = capgen.load_params(model, params)\n",
    "    tparams = capgen.init_tparams(params)\n",
    "\n",
    "    # word index\n",
    "    f_init, f_next = capgen.build_sampler(tparams, options, use_noise, trng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    trng, use_noise, \\\n",
    "          inps, alphas, alphas_samples, \\\n",
    "          cost, opt_outs = \\\n",
    "          capgen.build_model(tparams, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the alphas and selector value [called \\beta in the paper]\n",
    "\n",
    "# create update rules for the stochastic attention\n",
    "hard_attn_updates = []\n",
    "if options['attn_type'] == 'stochastic':\n",
    "    baseline_time = theano.shared(numpy.float32(0.), name='baseline_time')\n",
    "    hard_attn_updates += [(baseline_time, baseline_time * 0.9 + 0.1 * opt_outs['masked_cost'].mean())]\n",
    "    hard_attn_updates += opt_outs['attn_updates']\n",
    "    \n",
    "f_alpha = theano.function(inps, alphas, name='f_alpha', updates=hard_attn_updates)\n",
    "if options['selector']:\n",
    "    f_sels = theano.function(inps, opt_outs['selector'], name='f_sels', updates=hard_attn_updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Caption and Attention Visualization\n",
    "\n",
    "(The next five cells can be run over and over to visualize a random image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = numpy.random.randint(0, len(valid[0])) # random image\n",
    "#print(type(valid[1][valid[0][idx][1]].todense()))\n",
    "k = 1 # beam width\n",
    "use_gt = False # set to False if you want to use the generated sample\n",
    "gt = valid[0][idx][0] # groundtruth\n",
    "context = numpy.array(valid[1][valid[0][idx][1]]).reshape([14*14, 512]) # annotations\n",
    "img = LoadImage(image_path+flist[valid[0][idx][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not use_gt:\n",
    "    sample, score = capgen.gen_sample(tparams, f_init, f_next, context, \n",
    "                                      options, trng=trng, k=k, maxlen=200, stochastic=False)\n",
    "    sidx = numpy.argmin(score)\n",
    "    caption = sample[sidx][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: A man playing a guitar with a microphone\n",
      "GT: A man is playing an instrument trying to make money with his case open\n"
     ]
    }
   ],
   "source": [
    "# print the generated caption and the ground truth\n",
    "if use_gt:\n",
    "    caption = map(lambda w: worddict[w] if worddict[w] < options['n_words'] else 1, gt.split())\n",
    "words =list( map(lambda w: word_idict[w] if w in word_idict else '<UNK>', caption))\n",
    "print ('Sample:', ' '.join(words))\n",
    "print ('GT:', gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = f_alpha(numpy.array(caption).reshape(len(caption),1), \n",
    "                numpy.ones((len(caption),1), dtype='float32'), \n",
    "                context.reshape(1,context.shape[0],context.shape[1]))\n",
    "if options['selector']:\n",
    "    sels = f_sels(numpy.array(caption).reshape(len(caption),1), \n",
    "                   numpy.ones((len(caption),1), dtype='float32'), \n",
    "                   context.reshape(1,context.shape[0],context.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the Visualization   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'map' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-69716d8834ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mii\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mlab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mii\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'selector'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlab\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'(%0.2f)'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0msels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mii\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'map' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# display the visualization\n",
    "n_words = alpha.shape[0] + 1\n",
    "w = numpy.round(numpy.sqrt(n_words))\n",
    "h = numpy.ceil(numpy.float32(n_words) / w)\n",
    "        \n",
    "plt.subplot(w, h, 1)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "\n",
    "smooth = True\n",
    "\n",
    "for ii in range(alpha.shape[0]):\n",
    "    plt.subplot(w, h, ii+2)\n",
    "    lab = words[ii]\n",
    "    if options['selector']:\n",
    "        lab += '(%0.2f)'%sels[ii]\n",
    "    plt.text(0, 1, lab, backgroundcolor='white', fontsize=13)\n",
    "    plt.text(0, 1, lab, color='black', fontsize=13)\n",
    "    plt.imshow(img)\n",
    "    if smooth:\n",
    "        alpha_img = skimage.transform.pyramid_expand(alpha[ii,0,:].reshape(14,14), upscale=16, sigma=20)\n",
    "    else:\n",
    "        alpha_img = skimage.transform.resize(alpha[ii,0,:].reshape(14,14), [img.shape[0], img.shape[1]])\n",
    "    plt.imshow(alpha_img, alpha=0.8)\n",
    "    plt.set_cmap(cm.Greys_r)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
